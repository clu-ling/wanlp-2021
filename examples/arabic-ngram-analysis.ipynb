{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                            as pd\n",
    "import numpy                             as np\n",
    "from typing                          import List\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base                    import BaseEstimator\n",
    "from sklearn.preprocessing           import LabelEncoder, LabelBinarizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.metrics                 import classification_report\n",
    "\n",
    "\n",
    "from classify_dialects               import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic $n$-gram analysis\n",
    "\n",
    "Calculate the $k$ most informative/predictive $n$-grams per dialect class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0: Install CAMeL Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install camel-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!camel_data full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: define our tokenizer using a morphological analyzer from the CAMeL Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.morphology.database      import MorphologyDB\n",
    "from camel_tools.morphology.analyzer      import Analyzer\n",
    "from camel_tools.disambig.mle             import MLEDisambiguator\n",
    "\n",
    "# see https://camel-tools.readthedocs.io/en/latest/api/morphology/analyzer.html#camel_tools.morphology.analyzer.Analyzer\n",
    "\n",
    "db            = MorphologyDB.builtin_db()\n",
    "\n",
    "# Create analyzer with no backoff\n",
    "#analyzer       = Analyzer(db)\n",
    "\n",
    "# Create analyzer with NOAN_PROP backoff\n",
    "analyzer      = Analyzer(db, backoff='NOAN_PROP')\n",
    "\n",
    "disambiguator = MLEDisambiguator(analyzer, mle_path=None)\n",
    "segmenter     = MorphologicalTokenizer(disambiguator, split=True, diac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"بعد ما أخيرا زبط نومي بعد الnight rotation ونمت عال٩ اليوم بصحى عال٣ الصبح\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda text: segmenter.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"data/DA_train_labeled.tsv\"\n",
    "train_df      = load_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#1_tweetid', '#2_tweet', '#3_country_label', '#4_province_label'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_column = \"#2_tweet\"\n",
    "y_column = \"#3_country_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[[x_column, y_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Morocco', 'Mauritania', 'Syria', 'Tunisia', 'Kuwait', 'Palestine', 'Oman', 'Jordan', 'Djibouti', 'United_Arab_Emirates', 'Somalia', 'Sudan', 'Yemen', 'Qatar', 'Algeria', 'Lebanon', 'Saudi_Arabia', 'Egypt', 'Bahrain', 'Libya', 'Iraq'}\n"
     ]
    }
   ],
   "source": [
    "labels = set(train_df['#3_country_label'])\n",
    "print(labels)\n",
    "#regions   = set(train_df['#4_province_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Preprocess our data and transform text as $n$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizer = CountVectorizer\n",
    "Vectorizer = TfidfVectorizer\n",
    "\n",
    "custom_vectorizer = Vectorizer(\n",
    "    #analyzer=tokenize, \n",
    "    tokenizer=tokenize,\n",
    "    binary=False, \n",
    "    ngram_range=(3,3)#(1, 3)\n",
    ")\n",
    "\n",
    "le        = LabelEncoder()\n",
    "# calculate (thresholded) n-gram counts based on all training data\n",
    "X         = custom_vectorizer.fit_transform(train_df[x_column])\n",
    "y         = le.fit_transform(train_df[y_column].values)\n",
    "\n",
    "\n",
    "# feature ID -> feature name\n",
    "custom_vectorizer.id2feat = {i:f for (f,i) in custom_vectorizer.vocabulary_.items()}\n",
    "\n",
    "# convenience method\n",
    "label2id = lambda lbl: le.transform([lbl])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: fit a classifier for each class (i.e., a series of binary classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for label in labels:\n",
    "    # binarize classes in 1 v. rest fashion\n",
    "    y_subset   = np.array(y, copy=True)\n",
    "    label_id   = label2id(label)\n",
    "    y_subset[y_subset != label_id] = -1\n",
    "    y_subset[y_subset == label_id] = 1\n",
    "    #print(f\"label: {label}\")\n",
    "    clf        = LogisticRegression()\n",
    "    clf.fit(X, y_subset)\n",
    "    res[label] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Calculate the top $k$ most informative features based on the learned model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_features(clf: LogisticRegression, n: int) -> np.ndarray:\n",
    "    return np.argsort(clf.coef_[0])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for(indices: np.ndarray, vectorizer: BaseEstimator = custom_vectorizer)  -> List[str]:\n",
    "    return [vectorizer.id2feat[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_for(clf: LogisticRegression, n: int, vectorizer: BaseEstimator = custom_vectorizer) -> List[str]:\n",
    "    feature_ids = get_top_n_features(clf, n)\n",
    "    return get_features_for(feature_ids, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1826,  1586, 25335, 22355, 26011, 28793, 11730, 22907,  1554,\n",
       "       14913])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_features(res[\"Iraq\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  وَ  '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_vectorizer.id2feat[get_top_n_features(res[\"Iraq\"], n=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  وَ  ',\n",
       " '  ف ي',\n",
       " 'م ش  ',\n",
       " 'ف ي  ',\n",
       " 'ن أَ  ',\n",
       " 'ي أَ  ',\n",
       " 'أَ ن أَ',\n",
       " 'ق وَ لِ',\n",
       " '  ف  ',\n",
       " 'ح أَ ج']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_for(res[\"Iraq\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ه ه ه',\n",
       " '  بِ ي',\n",
       " 'م أَ لِ',\n",
       " 'م ش  ',\n",
       " 'ه   أَ',\n",
       " 'أَ ي ه',\n",
       " '  ف ي',\n",
       " 'أَ ت  ',\n",
       " 'ف ي  ',\n",
       " '  u r']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_for(res[\"Kuwait\"], n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

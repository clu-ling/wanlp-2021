{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Level: Aravec Word embeddings and CNNs, RNNs\n",
    "\n",
    "**Subtask 1**: Country-level dialect identification: A total of 21,000 tweets, covering all 21 Arab countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "def read_files(path):\n",
    "    file = pd.read_csv(path, sep='\\t')\n",
    "    print ('The shape of the data: ', file.shape)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data:  (21000, 4)\n",
      "The shape of the data:  (5000, 4)\n",
      "The shape of the data:  (5000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_tweetid</th>\n",
       "      <th>#2_tweet</th>\n",
       "      <th>#3_country_label</th>\n",
       "      <th>#4_province_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0</td>\n",
       "      <td>حاجة حلوة اكيد</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>eg_Faiyum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_1</td>\n",
       "      <td>عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشو...</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>iq_Dihok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_2</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>Saudi_Arabia</td>\n",
       "      <td>sa_Ha'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_3</td>\n",
       "      <td>منطق 2017: أنا والغريب علي إبن عمي وأنا والغري...</td>\n",
       "      <td>Mauritania</td>\n",
       "      <td>mr_Nouakchott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_4</td>\n",
       "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>dz_El-Oued</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>TRAIN_20995</td>\n",
       "      <td>هذا أناني و نافخ روحو</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>dz_Ouargla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>TRAIN_20996</td>\n",
       "      <td>ابا أتعلم ارسم  URL  …</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>iq_Basra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>TRAIN_20997</td>\n",
       "      <td>كلمة وقح شكلك توك الا متعلمتنها كثير تقوليها ب...</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>iq_Basra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>TRAIN_20998</td>\n",
       "      <td>ربنا ما يوريك الناس الدحيحة لما يترَوشِنوا</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>eg_Gharbia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>TRAIN_20999</td>\n",
       "      <td>حرام يا منى ليه بس السيره دي ع الصبح</td>\n",
       "      <td>Lebanon</td>\n",
       "      <td>lb_South-Lebanon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        #1_tweetid                                           #2_tweet  \\\n",
       "0          TRAIN_0                                     حاجة حلوة اكيد   \n",
       "1          TRAIN_1  عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشو...   \n",
       "2          TRAIN_2                                      ابشر طال عمرك   \n",
       "3          TRAIN_3  منطق 2017: أنا والغريب علي إبن عمي وأنا والغري...   \n",
       "4          TRAIN_4                  شهرين وتروح والباقي غير صيف ملينا   \n",
       "...            ...                                                ...   \n",
       "20995  TRAIN_20995                              هذا أناني و نافخ روحو   \n",
       "20996  TRAIN_20996                             ابا أتعلم ارسم  URL  …   \n",
       "20997  TRAIN_20997  كلمة وقح شكلك توك الا متعلمتنها كثير تقوليها ب...   \n",
       "20998  TRAIN_20998         ربنا ما يوريك الناس الدحيحة لما يترَوشِنوا   \n",
       "20999  TRAIN_20999               حرام يا منى ليه بس السيره دي ع الصبح   \n",
       "\n",
       "      #3_country_label #4_province_label  \n",
       "0                Egypt         eg_Faiyum  \n",
       "1                 Iraq          iq_Dihok  \n",
       "2         Saudi_Arabia          sa_Ha'il  \n",
       "3           Mauritania     mr_Nouakchott  \n",
       "4              Algeria        dz_El-Oued  \n",
       "...                ...               ...  \n",
       "20995          Algeria        dz_Ouargla  \n",
       "20996             Iraq          iq_Basra  \n",
       "20997             Iraq          iq_Basra  \n",
       "20998            Egypt        eg_Gharbia  \n",
       "20999          Lebanon  lb_South-Lebanon  \n",
       "\n",
       "[21000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = read_files('../data/train_dev/Subtask_1.2+2.2_DA/DA_train_labeled.tsv')\n",
    "dev_df = read_files('../data/train_dev/Subtask_1.2+2.2_DA/DA_dev_labeled.tsv')\n",
    "test_df = read_files('../data/test/Subtask_1.2+2.2_DA/DA_test_unlabeled.tsv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "def normalize(text):\n",
    "    normalized = str(text)\n",
    "    normalized = re.sub('URL','',normalized) # remove links\n",
    "    normalized = re.sub('USER','',normalized) # remove USER\n",
    "    normalized = re.sub('#','',normalized) # remove #\n",
    "    #normalized = re.sub('(@[A-Za-z0-9]+)_[A-Za-z0-9]+','',normalized) # remove @names with underscore\n",
    "    #normalized = re.sub('(@[A-Za-z0-9]+)','',normalized) # remove @names\n",
    "    #normalized = re.sub('pic\\S+','',normalized) # remove pic.twitter.com links\n",
    "    normalized = re.sub('\\d+','',normalized) # remove numbers\n",
    "    normalized = re.sub('-','',normalized) # remove symbols - . /\n",
    "    normalized = re.sub('[a-zA-Z0-9]+','',normalized) # remove English words \n",
    "    normalized = re.sub('!','',normalized) # remove English words\n",
    "    normalized = re.sub(':','',normalized) # remove English words\n",
    "    normalized = re.sub('[()]','',normalized) # remove English words\n",
    "    normalized = re.sub('☻','',normalized) # remove English words\n",
    "    normalized = re.sub('[\"\"]','',normalized) # remove English words\n",
    "    normalized = re.sub('é','',normalized) # remove English words\n",
    "    normalized = re.sub('\\/','',normalized) # remove English words\n",
    "    normalized = re.sub('؟','',normalized) # remove English words\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:00<00:00, 48768.14it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 47435.61it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 49886.70it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['#2_tweet'] = train_df['#2_tweet'].progress_apply(lambda text: normalize(text))\n",
    "dev_df['#2_tweet'] = dev_df['#2_tweet'].progress_apply(lambda text: normalize(text))\n",
    "test_df['#2_tweet'] = test_df['#2_tweet'].progress_apply(lambda text: normalize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete some stopwords \n",
    "def clean(text):\n",
    "    stops = ['و','الله','في', 'على', 'عن', 'إلى', 'الى', 'من', 'ما', 'لا', 'انا', 'أنا']\n",
    "    text = str(text)\n",
    "    text = text.split()\n",
    "    text = [w for w in text if w not in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:00<00:00, 182242.57it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 180283.86it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 179778.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['#2_tweet'] = train_df['#2_tweet'].progress_apply(lambda text: clean(text))\n",
    "dev_df['#2_tweet'] = dev_df['#2_tweet'].progress_apply(lambda text: clean(text))\n",
    "test_df['#2_tweet'] = test_df['#2_tweet'].progress_apply(lambda text: clean(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           حاجة حلوة اكيد\n",
       "1        عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشو...\n",
       "2                                            ابشر طال عمرك\n",
       "3        منطق والغريب علي إبن عمي وأنا والغريب وإبن عمي...\n",
       "4                        شهرين وتروح والباقي غير صيف ملينا\n",
       "                               ...                        \n",
       "20995                                  هذا أناني نافخ روحو\n",
       "20996                                     ابا أتعلم ارسم …\n",
       "20997    كلمة وقح شكلك توك الا متعلمتنها كثير تقوليها ب...\n",
       "20998              ربنا يوريك الناس الدحيحة لما يترَوشِنوا\n",
       "20999                 حرام يا منى ليه بس السيره دي ع الصبح\n",
       "Name: #2_tweet, Length: 21000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['#2_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Train_X, Dev_X, Test_X\n",
    "\n",
    "# train X, val X, test X\n",
    "train_X = train_df[\"#2_tweet\"]\n",
    "dev_X = dev_df[\"#2_tweet\"]\n",
    "test_X = test_df[\"#2_tweet\"]\n",
    "\n",
    "# target values\n",
    "train_y = train_df['#3_country_label']\n",
    "#print (train_y)\n",
    "dev_y = dev_df['#3_country_label']\n",
    "#print (dev_y)\n",
    "\n",
    "train_X = train_X.astype(str)\n",
    "dev_X = dev_X.astype(str)\n",
    "test_X = test_X.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some varialbles to preprocess the data with keras\n",
    "# 1) embed_size: the length of each word vector\n",
    "embed_size = 300\n",
    "# 2) features: unique words to use\n",
    "max_features = 50000\n",
    "# 3) maxlen: max number of words to use\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_X)\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_X)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "X_train = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "X_dev = pad_sequences(dev_sequences, maxlen=maxlen)\n",
    "X_test = pad_sequences(test_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (21000, 21)\n"
     ]
    }
   ],
   "source": [
    "# encode y data labels\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_y)\n",
    "y_train = encoder.transform(train_y)\n",
    "y_dev = encoder.transform(dev_y)\n",
    "\n",
    "N_CLASSES = np.max(y_train) + 1\n",
    "N_CLASSES\n",
    "y_train = to_categorical(y_train, N_CLASSES)\n",
    "y_dev = to_categorical(y_dev, N_CLASSES)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The AraVec Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please wait ... loading the AraVec\n",
      "Word2Vec(vocab=1476715, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import itertools\n",
    "# load the AraVec model for Arabic word embeddings - twitter-CBOW (300 vector size)\n",
    "print ('please wait ... loading the AraVec')\n",
    "aravec_model = gensim.models.Word2Vec.load('../aravec/full_grams_cbow_300_twitter.mdl')\n",
    "print (aravec_model)\n",
    "my_dict = {}\n",
    "for index, key in enumerate(aravec_model.wv.vocab):\n",
    "    my_dict[key] = aravec_model.wv[key]\n",
    "    \n",
    "#print the first 2 words and their vectors    \n",
    "# N = 2\n",
    "# out = dict(itertools.islice(my_dict.items(), N))\n",
    "# print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((50000, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > 50000 - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = my_dict.get(word)\n",
    "        #print (embedding_vector)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            #print (len(embedding_matrix[index]))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Embedding\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(50000, 300, input_length=100, weights=[embedding_matrix], trainable=False))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Conv1D(64, 5, activation='relu'))\n",
    "model_1.add(MaxPooling1D(pool_size=4))\n",
    "model_1.add(LSTM(300))\n",
    "model_1.add(Dense(21, activation='softmax'))\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "420/420 [==============================] - 33s 75ms/step - loss: 2.3908 - accuracy: 0.3095\n",
      "Epoch 2/5\n",
      "420/420 [==============================] - 34s 82ms/step - loss: 2.0359 - accuracy: 0.4030\n",
      "Epoch 3/5\n",
      "420/420 [==============================] - 33s 79ms/step - loss: 1.8682 - accuracy: 0.4483\n",
      "Epoch 4/5\n",
      "420/420 [==============================] - 33s 78ms/step - loss: 1.7421 - accuracy: 0.4845\n",
      "Epoch 5/5\n",
      "420/420 [==============================] - 33s 78ms/step - loss: 1.6338 - accuracy: 0.5165\n"
     ]
    }
   ],
   "source": [
    "train = model_1.fit(X_train, y_train, epochs = 5, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 23ms/step\n",
      "Labels are:  [ 3  3  3 ... 11 11 14]\n",
      "The length of predicted labels is:  5000\n"
     ]
    }
   ],
   "source": [
    "#apply to validation set\n",
    "pred_dev_y = model_1.predict([X_dev], batch_size=50, verbose=1)\n",
    "pred_dev_y\n",
    "indexes = np.argsort(pred_dev_y)[::-1]\n",
    "indexes\n",
    "\n",
    "# labels for the predicted dev data\n",
    "labels = np.argmax(pred_dev_y, axis=-1)    \n",
    "print('Labels are: ',labels)\n",
    "\n",
    "# getting the labels throw (inverse_transform)\n",
    "dev_y_predicted = encoder.inverse_transform(labels)\n",
    "print ('The length of predicted labels is: ', len(dev_y_predicted))\n",
    "\n",
    "# save labels to txt file\n",
    "with open(\"../prediction_files/predicted_dev_labels_aravec_cbow_cnn_3.txt\", \"w\") as f:\n",
    "    for s in dev_y_predicted:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.7228674861858592\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 2.2544 - accuracy: 0.3774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.2543509006500244, 0.3774000108242035)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "output_test = model_1.predict(X_dev)\n",
    "print(\"test auc:\", roc_auc_score(y_dev,output_test))\n",
    "dev_loss, dev_acc = model_1.evaluate(X_dev, y_dev)\n",
    "dev_loss, dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# applying to testing unlabeled dataset\n",
    "pred_test_y = model_1.predict([X_test], batch_size=50, verbose=1)\n",
    "pred_test_y\n",
    "indexes = np.argsort(pred_dev_y)[::-1]\n",
    "#print (indexes)\n",
    "\n",
    "# labels for the predicted dev data\n",
    "labels = np.argmax(pred_test_y, axis=-1)    \n",
    "#print('Labels are: ',labels)\n",
    "\n",
    "# getting the labels throw (inverse_transform)\n",
    "test_y_predicted = encoder.inverse_transform(labels)\n",
    "#print ('The length of predicted labels is: ', len(test_y_predicted))\n",
    "\n",
    "# save labels to txt file\n",
    "with open(\"../prediction_files/predicted_test_labels_aravec_cbow_cnn_3.txt\", \"w\") as f:\n",
    "    for s in dev_y_predicted:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

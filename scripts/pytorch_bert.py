# -*- coding: utf-8 -*-
"""dialect_pytorch_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v64F7dhStNQOPrbRKYF-KFcBOVTO7Ypg
"""
# Commented out IPython magic to ensure Python compatibility.
import transformers
from transformers import AdamW, get_linear_schedule_with_warmup
from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader

import numpy as np
import pandas as pd
from tqdm import tqdm
tqdm.pandas()
import re
from sklearn.model_selection import train_test_split
from collections import defaultdict

import warnings

warnings.filterwarnings('ignore')

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# detect the gpu
if torch.cuda.is_available():    
    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

# read the data
train = pd.read_csv('DA_train_labeled.tsv', header =0, sep ='\t', names = ['id','tweet','country','province'], encoding='utf-8')
dev = pd.read_csv('DA_dev_labeled.tsv', header =0, sep ='\t', names = ['id','tweet','country','province'], encoding='utf-8')
test = pd.read_csv('DA_test_unlabeled.tsv', header =0, sep ='\t', names = ['id','tweet'], encoding='utf-8')

# clean data


def normalize(text: str) -> str:
  normalized = str(text)
  normalized = re.sub('URL', '', normalized)  # remove links
  normalized = re.sub('USER', '', normalized)  # remove USER
  normalized = re.sub('#', '', normalized)  # remove #
  #normalized = re.sub('(@[A-Za-z0-9]+)_[A-Za-z0-9]+','',normalized) # remove @names with underscore
  #normalized = re.sub('(@[A-Za-z0-9]+)','',normalized) # remove @names
  #normalized = re.sub('pic\S+','',normalized) # remove pic.twitter.com links
  normalized = re.sub('\d+', '', normalized)  # remove numbers
  normalized = re.sub('-', '', normalized)  # remove symbols - . /
  normalized = re.sub('[a-zA-Z0-9]+', '', normalized)  # remove English words
  normalized = re.sub('!', '', normalized)  # remove English words
  normalized = re.sub(':', '', normalized)  # remove English words
  normalized = re.sub('[()]', '', normalized)  # remove English words
  normalized = re.sub('[""]', '', normalized)  # remove English words
  normalized = re.sub('é', '', normalized)  # remove English words
  normalized = re.sub('\/', '', normalized)  # remove English words
  normalized = re.sub('؟', '', normalized)  # remove English words
  return normalized


# normalize text data
train.tweet = train.tweet.progress_apply(lambda text: normalize(text))
dev.tweet = dev.tweet.progress_apply(lambda text: normalize(text))
test.tweet = test.tweet.progress_apply(lambda text: normalize(text))


# Xs
train.tweet = train.tweet.astype('str')
dev.tweet = dev.tweet.astype('str')
test.tweet = test.tweet.astype('str')

# ys
y_train = train.country.tolist()
y_dev = dev.country.tolist()

# the pre-trained mode, max_len, batch_size and epochs
PRE_TRAINED_MODEL_NAME = 'asafaya/bert-base-arabic'
MAX_LEN = 160
BATCH_SIZE = 16
EPOCHS = 1

tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
# Bert Tokenizer with example
sample_txt = 'السلام عليكم ورحمه الله وبركاته يا شباب كيف حالكم لعلكم بخير وفي اتم صحة وعافية وأهلا وسهلا بكم في درس اليوم'

tokens = tokenizer.tokenize(sample_txt)
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print(f' Sentence: {sample_txt}')
print(f'   Tokens: {tokens}')
print(f'Token IDs: {token_ids}')

encoding = tokenizer.encode_plus(
    sample_txt,
    max_length=32,
    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
    return_token_type_ids=False,
    pad_to_max_length=True,
    return_attention_mask=True,
    return_tensors='pt',  # Return PyTorch tensors
)
print (encoding.keys())

countries = {'Egypt':0, 
              'Iraq':1, 
              'Saudi_Arabia':2, 
              'Algeria':3, 
              'Oman':4, 
              'Libya':5,
              'Syria':6,
              'Morocco':7,
              'Tunisia':8,
              'United_Arab_Emirates':9,
              'Lebanon':10,
              'Yemen':11,
              'Kuwait':12,
              'Jordan':13,
              'Palestine':14,
              'Somalia':15,
              'Mauritania':16,
              'Sudan':17,
              'Qatar':18,
              'Bahrain':19,
              'Djibouti':20}

# list of target labels 
targets = [countries[key] for key in y_train] 
print(targets[1:10])


# Preparing the dataset with input ids and attention_masks
class TweetsDataset(Dataset):
    """
    takes tweets and targets and returns a dictionary of
    tweets, input ids, attention mask, targets
    """
    def __init__(self, tweets, targets, tokenizer, max_len):
        self.tweets = tweets
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.tweets)
    def __getitem__(self, item):
        tweet = str(self.tweets[item])
        target = self.targets[item]
        encoding = self.tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False,
                                              pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True)
        return {'tweet': tweet, 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(),
                'targets': torch.tensor(target, dtype=torch.long)}


# d = TweetsDataset(tweets=train.tweet.to_numpy(), targets=np.asarray(
#     targets), tokenizer=tokenizer, max_len=MAX_LEN)

# print (type(d.tweets)) # type numpy.ndarray
# print(type(d.targets))  # type numpy.ndarray


# Splitting the training dataset for training and validation
df_train, df_val = train_test_split(train, test_size = 0.2, random_state = RANDOM_SEED)
####### required for training #########
df_train = df_train.astype('str')
df_val = df_val.astype('str')
print ("The shape of the training dataset : ", df_train.shape)
print ("The shape of the validation dataset : ", df_val.shape)
print ()
####### the original dev data and test data required for testing #########
X_dev = dev.astype('str')
X_test = test.astype('str')
print ("The shape of the original dev dataset : ", dev.shape)
print ("The shape of the test dataset : ", test.shape)

# Creating the data loader for training, validation and testing using the pytorch DataLoader
def create_data_loader(train, tokenizer, max_len, batch_size):
    d = TweetsDataset(tweets = train.tweet.to_numpy(), targets = np.asarray(targets), tokenizer = tokenizer, max_len = max_len)
    return DataLoader(d, batch_size = batch_size, num_workers = 0)

# for training and validation
train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
# original dev and test
dev_data_loader = create_data_loader(X_dev, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(X_test, tokenizer, MAX_LEN, BATCH_SIZE)

data = next(iter(train_data_loader))
print (data.keys())

print (len(train_data_loader))
print (len(val_data_loader))
print (len(dev_data_loader))
print (len(test_data_loader))

# Shape of the torch
print(data['input_ids'].shape)
print(data['attention_mask'].shape)
print(data['targets'].shape)
print(data['input_ids'][0])

# Using the Bert Model
bert_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
last_hidden_state, pooled_output = bert_model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'], return_dict=False)
print(last_hidden_state.shape)
print(pooled_output.shape)

print(type(bert_model.config.hidden_size))


# Model with BERT linear layer
class TweetsClassifier(nn.Module):
    def __init__(self, n_classes):
        super(TweetsClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        _, pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask, return_dict=False
        )
        output = self.drop(pooled_output)
        return self.out(output)


model = TweetsClassifier(21)
model = model.to(device)

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)


print(input_ids.shape) # batch size x seq length
print(attention_mask.shape) # batch size x seq length

model(input_ids, attention_mask)

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=total_steps)
loss_fn = nn.CrossEntropyLoss().to(device)


# Training function
def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):  
    # Putting the model in the training mode
    model = model.train()
    losses = []
    correct_predictions = 0
    for d in data_loader:
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        _, preds = torch.max(outputs, dim=1)
        loss = loss_fn(outputs, targets)
        correct_predictions += torch.sum(preds == targets)
        losses.append(loss.item())
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        
    return correct_predictions.double() / n_examples, np.mean(losses)


# Evaluation Functions
def eval_model(model, data_loader, loss_fn, device, n_examples):
    # Putting the model in the Evaluation mode
    model = model.eval()
    losses = []
    correct_predictions = 0
    with torch.no_grad():
        for d in data_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            targets = d["targets"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)
            loss = loss_fn(outputs, targets)
            correct_predictions += torch.sum(preds == targets)
            losses.append(loss.item())
            
    return correct_predictions.double() / n_examples, np.mean(losses)


# train
history = defaultdict(list)
best_accuracy = 0
for epoch in range(EPOCHS):
  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 70)
  train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))
  print(f'Train loss {train_loss} accuracy {train_acc}')
  val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))
  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()
  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)
  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model.pt')
    best_accuracy = val_acc

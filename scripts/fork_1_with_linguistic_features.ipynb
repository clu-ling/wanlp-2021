{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(32)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D,Flatten, concatenate, Dropout, Input, Embedding, Dense, Bidirectional\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction import DictVectorizer \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) embed_size: the length of each word vector\n",
    "embed_size = 300\n",
    "# 2) features: unique words to use\n",
    "max_features = 50000\n",
    "# 3) maxlen: max number of words to use\n",
    "maxlen = 100\n",
    "# the number of samples to use for one update\n",
    "batch = 64\n",
    "# the max number of epochs to use\n",
    "num_epochs = 2\n",
    "# data\n",
    "train = 'DA_train_labeled.tsv'\n",
    "dev = 'DA_dev_labeled.tsv'\n",
    "test = 'DA_test_unlabeled.tsv'\n",
    "w2v_data = 'cbow_100.bin'\n",
    "ling_size = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "def read_files(path):\n",
    "    file = pd.read_csv(path, sep='\\t')\n",
    "    print ('shape', file.shape)\n",
    "    return file\n",
    "\n",
    "train_df = read_files(train)\n",
    "dev_df = read_files(dev)\n",
    "test_df = read_files(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "def normalize(text):\n",
    "    normalized = str(text)\n",
    "    normalized = re.sub('URL','',normalized) # remove links\n",
    "    normalized = re.sub('USER','',normalized) # remove USER\n",
    "    normalized = re.sub('#','',normalized) # remove #\n",
    "    #normalized = re.sub('(@[A-Za-z0-9]+)_[A-Za-z0-9]+','',normalized) # remove @names with underscore\n",
    "    #normalized = re.sub('(@[A-Za-z0-9]+)','',normalized) # remove @names\n",
    "    #normalized = re.sub('pic\\S+','',normalized) # remove pic.twitter.com links\n",
    "    normalized = re.sub('\\d+','',normalized) # remove numbers\n",
    "    normalized = re.sub('-','',normalized) # remove symbols - . /\n",
    "    normalized = re.sub('[a-zA-Z0-9]+','',normalized) # remove English words \n",
    "    normalized = re.sub('!','',normalized) # remove English words\n",
    "    normalized = re.sub(':','',normalized) # remove English words\n",
    "    normalized = re.sub('[()]','',normalized) # remove English words\n",
    "    normalized = re.sub('☻','',normalized) # remove English words\n",
    "    normalized = re.sub('[\"\"]','',normalized) # remove English words\n",
    "    normalized = re.sub('é','',normalized) # remove English words\n",
    "    normalized = re.sub('\\/','',normalized) # remove English words\n",
    "    normalized = re.sub('؟','',normalized) # remove English words\n",
    "    return normalized\n",
    "\n",
    "train_df['#2_tweet'] = train_df['#2_tweet'].progress_apply(lambda text: normalize(text))\n",
    "dev_df['#2_tweet'] = dev_df['#2_tweet'].progress_apply(lambda text: normalize(text))\n",
    "test_df['#2_tweet'] = test_df['#2_tweet'].progress_apply(lambda text: normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    # AFRICA: Egypt\n",
    "    def egypt_neg(self, text):\n",
    "        return 1 if u'مش' in text else 0   \n",
    "    def egypt_dem(self, text):\n",
    "        return 1 if (u'ده' or u'دي' or 'دى') in text else 0\n",
    "    # AFRICA: Libya\n",
    "    def libya(self, text):\n",
    "        return 1 if u'شن' in text else 0\n",
    "    # AFRICA: Tunisia\n",
    "    def tunis(self, text):\n",
    "        return 1 if u'يطيشوه' in text else 0\n",
    "    def tunis_iterog(self, text):\n",
    "        return 1 if u'علاش' in text else 0\n",
    "    def tunis_degree(slef, text):\n",
    "        return 1 if u'برشا' in text else 0\n",
    "    def tunis_contextualword(self, text):\n",
    "        return 1 if u'باهي' in text else 0\n",
    "    # AFRICA: Algeria\n",
    "    def algeria(self, text):\n",
    "        return 1 if u'كاش' in text else 0\n",
    "    # AFRICA: Moroccoa\n",
    "    def mor_dem(self, text):\n",
    "        return 1 if (u'ديال' or u'ديالي' or 'ديالى') in text else 0\n",
    "    # AFRICA: Mauritania\n",
    "    def mauritania(self, text):\n",
    "        return 1 if (u'كاغ' or u'ايكد') in text else 0\n",
    "    # AFRICA: Sudan\n",
    "    def sudan(self, text):\n",
    "        return 1 if u'ياخ' in text else 0\n",
    "    # AFRICA: Di\n",
    "    def dijubuti(self, text):\n",
    "        return 1 if (u'هاد' or u'هلق') in text else 0\n",
    "    # AFRICA: Somalia\n",
    "    def somalia(self, text):\n",
    "        return 1 if u'تناطل' in text else 0\n",
    "    \n",
    "    ##########################\n",
    "    # ASIA: Iraq\n",
    "    def iraq_degree(self,text):\n",
    "        return 1 if (u'خوش' or u'كاعد') in text else 0\n",
    "    def iraq_dem(self, text):\n",
    "        return 1 if (u'هاي' or u'دا') in text else 0\n",
    "    def iraq_adj(self,text):\n",
    "        return 1 if (u'فدوه' or u'فدوة') in text else 0\n",
    "    def iraq_interrog(self, text):\n",
    "        return 1 if u'شديحس' in text else 0\n",
    "    def iraq_tensemarker(self, text):\n",
    "        return 1 if (u'هسه' or u'هسع' or u'لهسه')  in text else 0\n",
    "    # ASIA: Saudi\n",
    "    def saudi_dem(self, text):\n",
    "        return 1 if u'كذا' in text else 0\n",
    "    # ASIA: Qatar\n",
    "    def qatar(self, text):\n",
    "        return 1 if u'وكني' in text else 0\n",
    "    # ASIA: Bahrain\n",
    "    def bahrain(self, text):\n",
    "        return 1 if u'شفيها' in text else 0\n",
    "    # ASIA: UAE\n",
    "    def emirates(self, text):\n",
    "        return 1 if u'عساه' in text else 0\n",
    "    # ASIA: Kuwait\n",
    "    def kuwait(self, text):\n",
    "        return 1 if u'عندج' in text else 0\n",
    "    # ASIA: Oman\n",
    "    def oman(self, text):\n",
    "        return 1 if u'عيل' in text else 0\n",
    "    # ASIA: Yemen\n",
    "    def yemen(self, text):\n",
    "        return 1 if u'كدي' in text else 0\n",
    "    # ASIA: Syria\n",
    "    def syria(self, text):\n",
    "        return 1 if u'شنو' in text else 0\n",
    "    # ASIA: Palestine\n",
    "    def palestine(self, text):\n",
    "        return 1 if u'ليش' in text else 0\n",
    "    # ASIA: Jordan\n",
    "    def jordan(self, text):\n",
    "        return 1 if u'هاظ' in text else 0\n",
    "    # ASIA: Lebanon\n",
    "    def lebanon(self, text):\n",
    "        return 1 if u'هيدي' in text else 0\n",
    "\n",
    "    \n",
    "    # create feature dictionary collects the previous function\n",
    "    def create_feature_dict(self, text):\n",
    "        return{\n",
    "            \"egypt_neg\": self.egypt_neg(text),\n",
    "            \"egypt_dem\": self.egypt_dem(text),\n",
    "            \"libya\": self.libya(text),\n",
    "            \"tunis\": self.tunis(text),\n",
    "            \"tunis_iterog\": self.tunis_iterog(text),\n",
    "            \"tunis_degree\": self.tunis_degree(text),\n",
    "            \"tunis_contextualword\": self.tunis_contextualword(text),\n",
    "            \"algeria\": self.algeria(text),\n",
    "            \"mor_dem\": self.mor_dem(text),\n",
    "            \"mauritania\": self.mauritania(text),\n",
    "            \"sudan\": self.sudan(text),\n",
    "            \"dijubuti\": self.dijubuti(text),\n",
    "            \"somalia\": self.somalia(text),\n",
    "            \"iraq_degree\": self.iraq_degree(text),\n",
    "            \"iraq_dem\": self.iraq_dem(text),\n",
    "            \"iraq_adj\": self.iraq_adj(text),\n",
    "            \"iraq_interrog\": self.iraq_interrog(text),\n",
    "            \"iraq_tensemarker\": self.iraq_tensemarker(text),\n",
    "            \"saudi_dem\": self.saudi_dem(text),\n",
    "            \"qatar\": self.qatar(text),\n",
    "            \"bahrain\": self.bahrain(text),\n",
    "            \"emirates\": self.emirates(text),\n",
    "            \"kuwait\": self.kuwait(text),\n",
    "            \"oman\": self.oman(text),\n",
    "            \"yemen\": self.yemen(text),\n",
    "            \"syria\": self.syria(text),\n",
    "            \"palestine\":self.palestine(text),\n",
    "            \"jordan\": self.jordan(text),\n",
    "            \"lebanon\": self.lebanon(text) \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncoder(object):\n",
    "    def __init__(self):\n",
    "        self.dv = DictVectorizer()\n",
    "\n",
    "    def fit_and_transform(self, texts):\n",
    "        m = self.dv.fit_transform(texts)\n",
    "        return m\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        feature_names = self.dv.get_feature_names()\n",
    "        return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linguistic_features(data):\n",
    "    feature_extraction = FeatureExtractor()\n",
    "    feature_reps = [] \n",
    "    for sent in data:\n",
    "        feature_dict = feature_extraction.create_feature_dict(sent)\n",
    "\n",
    "        feature_reps.append(list(feature_dict.values()))\n",
    "\n",
    "    linguistic_vector = np.array(feature_reps)\n",
    "    return linguistic_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Train_X, Dev_X, Test_X\n",
    "\n",
    "# train X, val X, test X\n",
    "train_X = train_df[\"#2_tweet\"]\n",
    "dev_X = dev_df[\"#2_tweet\"]\n",
    "test_X = test_df[\"#2_tweet\"]\n",
    "\n",
    "# target values\n",
    "train_y = train_df['#3_country_label']\n",
    "#print (train_y)\n",
    "dev_y = dev_df['#3_country_label']\n",
    "#print (dev_y)\n",
    "\n",
    "train_X = train_X.astype(str)\n",
    "dev_X = dev_X.astype(str)\n",
    "test_X = test_X.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_X)\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_X)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "X_train = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "X_dev = pad_sequences(dev_sequences, maxlen=maxlen)\n",
    "X_test = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "print (len(X_train))\n",
    "\n",
    "train_ling = linguistic_features(train_X)\n",
    "dev_ling = linguistic_features(dev_X)\n",
    "test_ling = linguistic_features(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode y data labels\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_y)\n",
    "y_train = encoder.transform(train_y)\n",
    "y_dev = encoder.transform(dev_y)\n",
    "\n",
    "N_CLASSES = np.max(y_train) + 1\n",
    "N_CLASSES\n",
    "y_train = to_categorical(y_train, N_CLASSES)\n",
    "y_dev = to_categorical(y_dev, N_CLASSES)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import itertools\n",
    "# load the AraVec model for Arabic word embeddings - twitter-CBOW (300 vector size)\n",
    "print ('please wait ... loading the AraVec')\n",
    "# load w2v model\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "w2v_vectors_file = './cbow_100.bin'\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_vectors_file, binary=True, unicode_errors='ignore')\n",
    "print (w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "for index, key in enumerate(w2v.wv.vocab):\n",
    "    my_dict[key] = w2v.wv[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((50000, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > 50000 - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = my_dict.get(word)\n",
    "        #print (embedding_vector)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            #print (len(embedding_matrix[index]))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "sss.get_n_splits(X_train, y_train)\n",
    "\n",
    "# for train_index, test_index in sss.split(X, y):\n",
    "# ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "# ...     X_train, X_test = X[train_index], X[test_index]\n",
    "# ...     y_train, y_test = y[train_index], y[test_index]\n",
    "# TRAIN: [5 2 3] TEST: [4 1 0]\n",
    "# TRAIN: [5 1 4] TEST: [0 2 3]\n",
    "# TRAIN: [5 0 2] TEST: [4 3 1]\n",
    "# TRAIN: [4 1 0] TEST: [2 3 5]\n",
    "# TRAIN: [0 5 1] TEST: [3 4 2]\n",
    "index = sss.split(X_train, y_train)\n",
    "for x in index:\n",
    "    train_index = x[0]\n",
    "    test_index = x[1]\n",
    "    print (len(x[0]))\n",
    "    print (len(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cnn model\n",
    "def cnn(length=maxlen, vocab_size=max_features):\n",
    "    # channel 1\n",
    "    inputs1 = Input(maxlen,)\n",
    "    embedding1 = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)(inputs1)\n",
    "    bidirectional1 = Bidirectional(LSTM(300, return_sequences=True))(embedding1)\n",
    "    drop1 = Dropout(0.5)(bidirectional1)\n",
    "    bidirectional2 = Bidirectional(LSTM(300))(drop1)\n",
    "    flat1 = Flatten()(bidirectional2)\n",
    "    # channel 2\n",
    "    inputs2 = Input(ling_size,)\n",
    "    dense = Dense(32, activation='relu')(inputs2)\n",
    "    drop2 = Dropout(0.5)(dense)\n",
    "   \n",
    "    # merge\n",
    "    merged = concatenate([flat1, drop2])\n",
    "    # interpretation\n",
    "    dense1 = Dense(332, activation='relu')(merged)\n",
    "    outputs = Dense(21, activation='softmax')(dense1) \n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model.fit([X_train[train_index], train_ling[train_index]], y_train[train_index],\n",
    "                  validation_data=([X_train[test_index], train_ling[test_index]], y_train[test_index]),\n",
    "                  epochs=10, batch_size=32, callbacks = [EarlyStopping(monitor='val_loss', patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply to validation set\n",
    "pred_dev_y = model.predict([X_dev, dev_ling], batch_size=50, verbose=1)\n",
    "pred_dev_y\n",
    "indexes = np.argsort(pred_dev_y)[::-1]\n",
    "indexes\n",
    "\n",
    "# labels for the predicted dev data\n",
    "labels = np.argmax(pred_dev_y, axis=-1)    \n",
    "print('Labels are: ',labels)\n",
    "\n",
    "# getting the labels throw (inverse_transform)\n",
    "dev_y_predicted = encoder.inverse_transform(labels)\n",
    "print ('The length of predicted labels is: ', len(dev_y_predicted))\n",
    "\n",
    "# save labels to txt file\n",
    "with open(\"two_forks_early.txt\", \"w\") as f:\n",
    "    for s in dev_y_predicted:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

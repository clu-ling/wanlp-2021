{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(32)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D,Flatten, concatenate, Dropout, Input, Embedding, Dense, Bidirectional\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) embed_size: the length of each word vector\n",
    "embed_size = 300\n",
    "# 2) features: unique words to use\n",
    "max_features = 50000\n",
    "# 3) maxlen: max number of words to use\n",
    "maxlen = 100\n",
    "# the number of samples to use for one update\n",
    "batch = 64\n",
    "# the max number of epochs to use\n",
    "num_epochs = 2\n",
    "# how many folds to use for cross validation\n",
    "folds = 10\n",
    "train = 'DA_train_labeled.tsv'\n",
    "dev = 'DA_dev_labeled.tsv'\n",
    "test = 'DA_test_unlabeled.tsv'\n",
    "w2v_data = 'cbow_100.bin'\n",
    "ling_size = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (21000, 4)\n",
      "shape (5000, 4)\n",
      "shape (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "def read_files(path):\n",
    "    file = pd.read_csv(path, sep='\\t')\n",
    "    print ('shape', file.shape)\n",
    "    return file\n",
    "\n",
    "train_df = read_files(train)\n",
    "dev_df = read_files(dev)\n",
    "test_df = read_files(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:00<00:00, 48838.12it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 49809.33it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 46411.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "def normalize(text):\n",
    "    normalized = str(text)\n",
    "    normalized = re.sub('URL','',normalized) # remove links\n",
    "    normalized = re.sub('USER','',normalized) # remove USER\n",
    "    normalized = re.sub('#','',normalized) # remove #\n",
    "    #normalized = re.sub('(@[A-Za-z0-9]+)_[A-Za-z0-9]+','',normalized) # remove @names with underscore\n",
    "    #normalized = re.sub('(@[A-Za-z0-9]+)','',normalized) # remove @names\n",
    "    #normalized = re.sub('pic\\S+','',normalized) # remove pic.twitter.com links\n",
    "    normalized = re.sub('\\d+','',normalized) # remove numbers\n",
    "    normalized = re.sub('-','',normalized) # remove symbols - . /\n",
    "    normalized = re.sub('[a-zA-Z0-9]+','',normalized) # remove English words \n",
    "    normalized = re.sub('!','',normalized) # remove English words\n",
    "    normalized = re.sub(':','',normalized) # remove English words\n",
    "    normalized = re.sub('[()]','',normalized) # remove English words\n",
    "    normalized = re.sub('☻','',normalized) # remove English words\n",
    "    normalized = re.sub('[\"\"]','',normalized) # remove English words\n",
    "    normalized = re.sub('é','',normalized) # remove English words\n",
    "    normalized = re.sub('\\/','',normalized) # remove English words\n",
    "    normalized = re.sub('؟','',normalized) # remove English words\n",
    "    return normalized\n",
    "\n",
    "train_df['#2_tweet'] = train_df['#2_tweet'].progress_apply(lambda text: normalize(text))\n",
    "dev_df['#2_tweet'] = dev_df['#2_tweet'].progress_apply(lambda text: normalize(text))\n",
    "test_df['#2_tweet'] = test_df['#2_tweet'].progress_apply(lambda text: normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizer is working:  <keras_preprocessing.text.Tokenizer object at 0x2aaca64a8>\n",
      "please wait ... loading the word embeddings\n",
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x2aaca69b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "Shape of label tensor: (21000, 21)\n",
      "Epoch 1/2\n",
      "329/329 [==============================] - 237s 710ms/step - loss: 2.2059 - acc: 0.3629\n",
      "Epoch 2/2\n",
      "329/329 [==============================] - 228s 693ms/step - loss: 1.6142 - acc: 0.5229\n",
      "100/100 [==============================] - 18s 171ms/step\n",
      "Labels are:  [ 3  8  3 ... 14 14 14]\n",
      "The length of predicted labels is:  5000\n"
     ]
    }
   ],
   "source": [
    "class DA(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = self.tokenize()\n",
    "        self.w2v, self.embedding_matrix = self.create_embeddings_matrix()\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.net = self.network()\n",
    "        \n",
    "    def tokenize(self):\n",
    "        tk = Tokenizer(num_words=max_features)\n",
    "        train_X = train_df[\"#2_tweet\"]\n",
    "        train_X = train_X.astype(str)\n",
    "        tk.fit_on_texts(train_X)\n",
    "        print ('\\ntokenizer is working: ', tk)\n",
    "        return tk\n",
    "    \n",
    "    def prepare_text (self, x):\n",
    "        tk = self.tokenizer\n",
    "        x = tk.texts_to_sequences(x)\n",
    "        x = pad_sequences(x, maxlen=maxlen)\n",
    "        return x\n",
    "    \n",
    "    def prepare_labels (self, y):\n",
    "        self.encoder.fit(y)\n",
    "        y = self.encoder.transform(y)\n",
    "        N_CLASSES = np.max(y) + 1\n",
    "        y = to_categorical(y, N_CLASSES)\n",
    "        print('Shape of label tensor:', y.shape)\n",
    "        return y\n",
    "    \n",
    "    def create_embeddings_matrix(self):\n",
    "        tk = self.tokenizer\n",
    "        print ('please wait ... loading the word embeddings')\n",
    "        w2v = KeyedVectors.load_word2vec_format(w2v_data, binary=True, unicode_errors='ignore')\n",
    "        print (w2v)\n",
    "        my_dict = {}\n",
    "        for index, key in enumerate(w2v.wv.vocab):\n",
    "            my_dict[key] = w2v.wv[key]\n",
    "        embedding_matrix = np.zeros((max_features, embed_size))\n",
    "        for word, index in tk.word_index.items():\n",
    "            if index > max_features - 1:\n",
    "                break\n",
    "            else:\n",
    "                embedding_vector = my_dict.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[index] = embedding_vector\n",
    "        print (embedding_matrix.shape)\n",
    "        return w2v, embedding_matrix\n",
    "    \n",
    "    def network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(max_features, \n",
    "                            embed_size, \n",
    "                            weights=[self.embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True))\n",
    "        #model.add(Bidirectional(LSTM(300, return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(300)))\n",
    "        model.add(Dense(21, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "        #print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def train(self):\n",
    "        model = self.net\n",
    "        train_text = self.prepare_text(train_df['#2_tweet'])\n",
    "        train_labels = self.prepare_labels(train_df['#3_country_label'])\n",
    "\n",
    "        model.fit(train_text, train_labels, epochs=num_epochs, verbose=1, batch_size=batch)\n",
    "        \n",
    "    def predict_dev(self):\n",
    "        model = self.net\n",
    "        dev_X = dev_df[\"#2_tweet\"]\n",
    "        dev_X = dev_X.astype(str)\n",
    "        dev_text = self.prepare_text(dev_X)\n",
    "        pred_dev_y = model.predict([dev_text], batch_size=50, verbose=1)\n",
    "\n",
    "        # labels for the predicted dev data\n",
    "        labels = np.argmax(pred_dev_y, axis=-1)  \n",
    "        print('Labels are: ',labels)\n",
    "\n",
    "        # getting the labels(inverse_transform)\n",
    "        dev_y_predicted = self.encoder.inverse_transform(labels)\n",
    "        print ('The length of predicted labels is: ', len(dev_y_predicted))\n",
    "\n",
    "        # save labels to txt file\n",
    "        with open(\"maxlen_60_2_epochs.txt\", \"w\") as f:\n",
    "            for s in dev_y_predicted:\n",
    "                f.write(str(s) +\"\\n\")\n",
    "        \n",
    "     \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    da = DA()\n",
    "    da.train()\n",
    "    da.predict_dev()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(32)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read the data\n",
    "def read_files(path):\n",
    "    file = pd.read_csv(path, sep='\\t')\n",
    "    print ('The shape of the data: ', file.shape)\n",
    "    return file\n",
    "\n",
    "train_df = read_files('DA_train_labeled.tsv')\n",
    "dev_df = read_files('DA_dev_labeled.tsv')\n",
    "test_df = read_files('DA_test_unlabeled.tsv')\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, dev, test\n",
    "train_X = train_df['#2_tweet']\n",
    "dev_X = dev_df['#2_tweet']\n",
    "test_X = test_df['#2_tweet']\n",
    "\n",
    "train_y = train_df['#3_country_label']\n",
    "dev_y = dev_df['#3_country_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 10000\n",
    "\n",
    "# get the raw text data\n",
    "train_X = train_X.astype(str)\n",
    "dev_X = dev_X.astype(str)\n",
    "test_X = test_X.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "sequences = tokenizer.texts_to_sequences(train_X)\n",
    "sequences_dev = tokenizer.texts_to_sequences(dev_X)\n",
    "sequences_test = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "tokenized sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = [len(s) for s in sequences]\n",
    "print(\"average length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"max length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=50);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad vectors to maximum length\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "# pad sequences with 0s\n",
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_dev = pad_sequences(sequences_dev, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data dev tensor:', x_dev.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode y data labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_y)\n",
    "y_train = encoder.transform(train_y)\n",
    "y_dev = encoder.transform(dev_y)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "N_CLASSES = np.max(y_train) + 1\n",
    "N_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, N_CLASSES,)\n",
    "y_dev = to_categorical(y_dev, N_CLASSES)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Simple Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = model.predict(x_dev)\n",
    "print(\"test auc:\", roc_auc_score(y_dev,output_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss, dev_acc = model.evaluate(x_dev, y_dev)\n",
    "dev_loss, dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a prediction on individual examples\n",
    "text_labels = encoder.classes_ \n",
    "\n",
    "for i in range(50,80):\n",
    "    prediction = model.predict(np.array([x_dev[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(dev_X.iloc[i], \"...\")\n",
    "    print('Actual label:' + dev_y.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = LSTM(128, dropout=0.5, recurrent_dropout=0.2)(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = model.predict(x_dev)\n",
    "print(\"test auc:\", roc_auc_score(y_dev,output_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_dev, y_dev,\n",
    "                       batch_size=64, verbose=1)\n",
    "print('Dev loss:', score[0])\n",
    "print('Dev accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# 1D convolution with 64 output channels\n",
    "x = Conv1D(64, 5)(embedded_sequences)\n",
    "# MaxPool divides the length of the sequence by 5\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv1D(64, 5)(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "# LSTM layer with a hidden size of 64\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(64)(x)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_dev, y_dev,\n",
    "                       batch_size=64, verbose=1)\n",
    "print('Dev loss:', score[0])\n",
    "print('Dev accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = model.predict(x_dev)\n",
    "print(\"dev auc:\", roc_auc_score(y_dev,output_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_ \n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(dev_X.iloc[i], \"...\")\n",
    "    print('Actual label:' + dev_y.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
